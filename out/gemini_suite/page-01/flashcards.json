[
  {
    "q": "RAW",
    "a": "Q1: Why do certain characters like O, A tend to survive during mode collapse in neural networks?\nA1: Their sharper edges make them easier for models to learn.\n\nQ2: Which generative adversarial network (GAN) maintains better letter diversity and coverage after a fix is applied according to the homework 2 context provided by Vivin Thiyagarajan, EE641?\nA2: The feature-matching GAN model.\n\nQ3: Why do letters like X, Y, Z still appear in some instances even when mode collapse occurs?\nA3: Because the character set does not completely fail to recognize them during collapses; they are less likely to be excluded than others with smoother shapes.\n\nQ4: What is one of the main reasons for using a feature-matching technique in GANs as opposed to just increasing model capacity?\nA4: To handle issues like mode collapse more effectively, by ensuring that important features are retained during training phases which helps maintain letter diversity and visual quality.\n\nQ5: What quantitative improvement does the homework suggest is observed when comparing a feature-matching GAN to a vanilla GAN baseline?\nA5: Feature-matching GAN shows improved coverage from ~0.7 (vanilla GAN) up to 0.8 and maintains better letter diversity, along with higher visual quality of the generated characters as evidenced by Figure 2 in Vivin Thiyagarajan's homework document EE641: Analysis Part II \u2013 Problem #1 - Mode Collapse & Diversity Losses (Problem Set I)."
  }
]